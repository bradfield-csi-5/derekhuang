#!/usr/bin/env bash

set -euo pipefail

BASE_URL="https://en.wikipedia.org/w/api.php"
PARSE_URL="${BASE_URL}?action=parse&format=json&formatversion=2"
QUERY_URL="${BASE_URL}?action=query&format=json&formatversion=2&prop=extracts&exintro&explaintext"
SEARCH_URL="${BASE_URL}?action=opensearch&limit=1"

script=$(basename "$0")

function usage {
  # Display usage and exit
  echo "Usage: ${script} PAGE [SECTION...] [SUBSECTION...]" >&2
  echo
  echo 'Display summary information from Wikipedia.' >&2
  exit 1
}

page=${1:-"NOPAGE!"}
if [ "${page}" == "NOPAGE!" ]; then
  echo "page is required" >&2
  echo
  usage
fi

function page_exists {
  # Search queries always return a 200 with the fourth array element
  # as an array of urls to pages if any exist
  # echo "page req: curl -s --url-query \"search=${page}\" \"${SEARCH_URL}\""
  # TODO: use page from response
  if [ "$(curl -s --url-query "search=${page}" "${SEARCH_URL}" | jq '.[3] | length > 0')" == "true" ]; then
    return 0
  fi
  return 1
}

# Short circuit if no pages found
if ! page_exists; then
  echo "Page ${page} not found" >&2
  usage
fi

# Print the first sentence
echo "Wikipedia page: '${page}'"
# echo "wiki req: curl -s --url-query \"titles=${page}\" \"${QUERY_URL}&redirects\""
intro=$(curl -s --url-query "titles=${page}" "${QUERY_URL}&redirects" | jq -r '.query.pages[0].extract | split(". ") | .[0]')
echo "  $intro"
echo

# Print section headings
echo "Sections:"
# echo "sections req: curl -s --url-query \"page=${page}\" \"${PARSE_URL}&prop=sections&redirects\""
IFS=$'\n'
for sect in $(curl -s --url-query "page=${page}" "${PARSE_URL}&prop=sections&redirects" | jq '.parse.sections | map_values(select(.toclevel == 1))' | jq -r '.[].line'); do
  echo "  $sect"
done
unset IFS

# TODO: handle subsections
